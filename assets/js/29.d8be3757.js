(window.webpackJsonp=window.webpackJsonp||[]).push([[29],{281:function(t,e,a){t.exports=a.p+"assets/img/SSW_1.0fac4fb4.png"},321:function(t,e,a){t.exports=a.p+"assets/img/SSW_2.34dd4016.png"},344:function(t,e,a){"use strict";a.r(e);var s=a(11),i=Object(s.a)({},(function(){var t=this,e=t._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("head",[e("link",{attrs:{href:"http://cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css",rel:"stylesheet"}})]),t._v(" "),e("section",{staticClass:"hero"},[e("div",{staticClass:"hero-body"},[e("div",{staticClass:"container is-max-desktop"},[e("div",{staticClass:"columns is-centered"},[e("div",{staticClass:"column has-text-centered"},[e("h1",{staticClass:"title is-1 publication-title"},[t._v("Deep Neural Network Watermarking against Model Extraction Attack")]),t._v(" "),e("h2",{staticClass:"title is-4 publication-title"},[t._v("ACM MM 2023")]),t._v(" "),e("div",{staticClass:"is-size-5 publication-authors"},[e("span",{staticClass:"author-block"},[t._v("\n              Jingxuan Tan"),e("sup",[t._v("1,2")]),t._v(", ")]),t._v(" "),e("span",{staticClass:"author-block"},[t._v("\n              Nan Zhong"),e("sup",[t._v("1,2")]),t._v(", ")]),t._v(" "),e("span",{staticClass:"author-block"},[t._v("\n              Zhenxing Qian"),e("sup",[t._v("1,2*")]),t._v(", \n            ")]),t._v(" "),e("span",{staticClass:"author-block"},[t._v("\n              Xinpeng Zhang"),e("sup",[t._v("1,2*")]),t._v(", \n            ")]),t._v(" "),e("span",{staticClass:"author-block"},[t._v("\n              Sheng Li"),e("sup",[t._v("1,2")])])]),t._v(" "),e("div",{staticClass:"is-size-5 publication-authors"},[e("div",[e("span",{staticClass:"author-block"},[e("sup",[t._v("1")]),t._v("School of Computer Science, Fudan University")])]),t._v(" "),e("div",[e("span",{staticClass:"author-block"},[e("sup",[t._v("2")]),t._v("Key Laboratory of Culture Tourism Intelligent Computing, Fudan University")])])]),t._v(" "),e("div",{staticClass:"column has-text-centered"},[e("div",{staticClass:"publication-links"},[e("span",{staticClass:"link-block"},[e("a",{staticClass:"external-link button is-normal is-rounded is-dark",attrs:{href:"https://github.com/jxtalent/SSW-DNN-Watermark"}},[e("span",{staticClass:"icon"},[e("i",{staticClass:"fab fa-github"})]),t._v(" "),e("span",[t._v("Code")])])])])])])])])])]),t._v(" "),e("section",{staticClass:"hero"},[e("div",{staticClass:"hero-body"},[e("div",{staticClass:"container is-max-desktop"},[e("div",{staticClass:"columns is-centered"},[e("div",{staticClass:"column has-text-centered"},[e("img",{attrs:{src:a(281)}}),t._v(" "),e("h2",{staticClass:"subtitle has-text-centered"},[e("span",{staticClass:"dnerf"},[t._v("The host model and its stolen copies output the same prediction on the trigger set")])]),t._v(" "),e("h2",{staticClass:"title publication-title"},[t._v("Abstract")])]),t._v(" "),e("p",[t._v("\n                Deep neural network (DNN) watermarking is an emerging technique to protect the intellectual property of deep learning models. At present, many DNN watermarking algorithms have been proposed to achieve provenance verification by embedding identify information into the internals or prediction behaviors of the host model. However, most methods are vulnerable to model extraction attacks, where attackers collect output labels from the model to train a surrogate or a replica. To address this issue, we present a novel DNN watermarking approach, named SSW, which constructs an adaptive trigger set progressively by optimizing over a pair of symmetric shadow models to enhance the robustness to model extraction. Precisely, we train a positive shadow model supervised by the prediction of the host model to mimic the behaviors of potential surrogate models. Additionally, a negative shadow model is normally trained to imitate irrelevant independent models. Using this pair of shadow models as a reference, we design a strategy to update the trigger samples appropriately such that they tend to persist in the host model and its stolen copies. Moreover, our method could well support two specific embedding schemes: embedding the watermark via fine-tuning or from scratch. Our extensive experimental results on popular datasets demonstrate that our SSW approach outperforms state-of-the-art methods against various model extraction attacks in whether trigger set classification accuracy based or hypothesis test-based verification. The results also show that our method is robust to common model modification schemes including fine-tuning and model compression.\n                ")]),t._v(" "),e("div",{staticClass:"column has-text-centered"},[e("h2",{staticClass:"title publication-title"},[t._v("Method")]),t._v(" "),e("img",{attrs:{src:a(321)}}),t._v(" "),e("h2",{staticClass:"subtitle has-text-centered"},[e("span",{staticClass:"dnerf"},[t._v("The overall pipeline of SSW")])])]),t._v(" "),e("p",[t._v("\n                    SSW algorithm involves three stages: watermark embedding, trigger selection, and ownership demonstration. In the watermark embedding stage, the host model "),e("common-latexDisplay",[t._v("H")]),t._v(" is trained on the union of the legitimate training data "),e("common-latexDisplay",[t._v("\\mathcal{D}")]),t._v(" and a trigger set "),e("common-latexDisplay",[t._v("\\mathcal{T}")]),t._v(". To make surrogate models derived from the host model output the same prediction on the trigger set, we optimize the trigger samples to make them actively adapt to surrogate models. To this end, we train a positive shadow model "),e("common-latexDisplay",[t._v("P")]),t._v(" on "),e("common-latexDisplay",[t._v("\\mathcal{D}^\\prime")]),t._v(", the data labeled by the host model, to simulate practical surrogate models. We also introduce a negative shadow model "),e("common-latexDisplay",[t._v("ùëÅ")]),t._v(" on "),e("common-latexDisplay",[t._v("\\mathcal{D}")]),t._v(" to represent the non-watermarked model. The trigger samples are optimized so that they are predicted to the same pre-defined label by "),e("common-latexDisplay",[t._v("H")]),t._v(" and "),e("common-latexDisplay",[t._v("P")]),t._v(" but to different labels by "),e("common-latexDisplay",[t._v("N")]),t._v(". The host model training, positive shadow model training, and trigger set optimization are alternately conducted to enhance the watermark robustness against model extraction.\n                ")],1),t._v(" "),e("p",[t._v("\n                    The trigger selection stage further selects samples that are more eligible for ownership verification. The ownership demonstration stage can be completed based on the classification accuracy on the trigger set or hypothesis test, depending on actual scenarios.\n                ")])])])])]),t._v(" "),e("section",{staticClass:"section"},[e("div",{staticClass:"container is-max-desktop"},[e("p",[t._v("If you like the project, please show your support by "),e("a",{attrs:{href:"https://github.com/jxtalent/SSW-DNN-Watermark"}},[t._v("leaving a star")]),t._v(" üåü !")])])]),t._v(" "),e("section",{staticClass:"section",attrs:{id:"BibTeX"}},[e("div",{staticClass:"container is-max-desktop content"},[e("h2",{staticClass:"title publication-title"},[t._v("BibTeX")]),t._v(" "),e("pre",[t._v("        "),e("code",[t._v("\n        @inproceedings{tan2023deep,\n        title={Deep Neural Network Watermarking against Model Extraction Attack},\n        author={Tan, Jingxuan and Zhong, Nan and Qian, Zhenxing and Zhang Xinpeng and Li Sheng},\n        booktitle={Proceedings of the 31st ACM International Conference on Multimedia},\n        year={2023}\n        }\n        ")]),t._v("\n    ")])])])])}),[],!1,null,null,null);e.default=i.exports}}]);