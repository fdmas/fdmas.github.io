(window.webpackJsonp=window.webpackJsonp||[]).push([[28],{280:function(e,t,a){e.exports=a.p+"assets/img/RetouchingFFHQ_1.4ab38dd8.png"},320:function(e,t,a){e.exports=a.p+"assets/img/RetouchingFFHQ_2.d5307157.png"},345:function(e,t,a){"use strict";a.r(t);var i=a(11),n=Object(i.a)({},(function(){var e=this,t=e._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[t("head",[t("link",{attrs:{href:"http://cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css",rel:"stylesheet"}})]),e._v(" "),t("section",{staticClass:"hero"},[t("div",{staticClass:"hero-body"},[t("div",{staticClass:"container is-max-desktop"},[t("div",{staticClass:"columns is-centered"},[t("div",{staticClass:"column has-text-centered"},[t("h1",{staticClass:"title is-1 publication-title"},[e._v("RetouchingFFHQ: A Large-scale Dataset for Fine-grained Face Retouching Detection")]),e._v(" "),t("h2",{staticClass:"title is-4 publication-title"},[e._v("ACM MM 2023")]),e._v(" "),t("div",{staticClass:"is-size-5 publication-authors"},[t("span",{staticClass:"author-block"},[e._v("\n              Qichao Ying"),t("sup",[e._v("1")]),e._v(", ")]),e._v(" "),t("span",{staticClass:"author-block"},[e._v("\n              Jiaxin Liu"),t("sup",[e._v("1")]),e._v(", ")]),e._v(" "),t("span",{staticClass:"author-block"},[e._v("\n              Sheng Li"),t("sup",[e._v("1,â˜…")]),e._v(", \n            ")]),e._v(" "),t("span",{staticClass:"author-block"},[e._v("\n              Haisheng Xu"),t("sup",[e._v("2")]),e._v(", \n            ")]),e._v(" "),t("span",{staticClass:"author-block"},[e._v("\n              Zhenxing Qian"),t("sup",[e._v("1,*")])]),e._v(" "),t("span",{staticClass:"author-block"},[e._v("\n              Xinpeng Zhang"),t("sup",[e._v("1")])])]),e._v(" "),t("div",{staticClass:"is-size-5 publication-authors"},[t("div",[t("span",{staticClass:"author-block"},[t("sup",[e._v("1")]),e._v("School of Computer Science, Fudan University, Shanghai, China")])]),e._v(" "),t("div",[t("span",{staticClass:"author-block"},[t("sup",[e._v("2")]),e._v("NVIDIA, Shanghai, China")])])]),e._v(" "),t("div",{staticClass:"column has-text-centered"},[t("div",{staticClass:"publication-links"},[t("span",{staticClass:"link-block"},[t("a",{staticClass:"external-link button is-normal is-rounded is-dark",attrs:{href:"https://arxiv.org/pdf/2307.10642"}},[t("span",{staticClass:"icon"},[t("i",{staticClass:"fas fa-file-pdf"})]),e._v(" "),t("span",[e._v("Paper")])])]),e._v(" "),t("span",{staticClass:"link-block"},[t("a",{staticClass:"external-link button is-normal is-rounded is-dark",attrs:{href:"https://github.com/yingqichao/retouching_FFHQ_detection"}},[t("span",{staticClass:"icon"},[t("i",{staticClass:"fab fa-github"})]),e._v(" "),t("span",[e._v("Code")])])])]),e._v(" "),t("p",[e._v("\n            Dataset Aquisition Click "),t("a",{attrs:{href:"/Application_RetouchingFFHQ_new.pdf"}},[e._v('"Application Form"')]),e._v(", fill in necessary information and send the PDF to fudanmaslab@gmail.com, with haoyuewang23@m.fudan.edu.cn and lisheng@fudan.edu.cn copied. Thank you!\n          ")])])])])])])]),e._v(" "),t("section",{staticClass:"hero"},[t("div",{staticClass:"hero-body"},[t("div",{staticClass:"container is-max-desktop"},[t("div",{staticClass:"columns is-centered"},[t("div",{staticClass:"column has-text-centered"},[t("img",{attrs:{src:a(280)}}),e._v(" "),t("h2",{staticClass:"subtitle has-text-centered"},[t("span",{staticClass:"dnerf"},[e._v("Examples of RetouchingFFHQ, a fine-grained face retouching dataset containing over half a million images")])]),e._v(" "),t("h2",{staticClass:"title publication-title"},[e._v("Abstract")])]),e._v(" "),t("p",[e._v("\n                The widespread use of face retouching filters on short-video platforms has raised concerns about the authenticity of digital appearances and the impact of deceptive advertising. To address these issues, there is a pressing need to develop advanced face retouching techniques. However, the lack of large-scale and fine-grained face retouching datasets has been a major obstacle to progress in this field. In this paper, we introduce RetouchingFFHQ, a large-scale and fine-grained face retouching dataset that contains over half a million conditionally-retouched images. RetouchingFFHQ stands out from previous datasets due to its large scale, high quality, fine-grainedness, and customization. By including four typical types of face retouching operations and different retouching levels, we extend the binary face retouching detection into a fine-grained, multi-retouching type, and multi-retouching level estimation problem. Additionally, we propose a Multi-granularity Attention Module (MAM) as a plugin for CNN backbones for enhanced cross-scale representation learning. Extensive experiments using different baselines as well as our proposed method on RetouchingFFHQ show decent performance on face retouching detection.\n                ")]),e._v(" "),t("div",{staticClass:"column has-text-centered"},[t("h2",{staticClass:"title publication-title"},[e._v("Method")]),e._v(" "),t("img",{attrs:{src:a(320)}}),e._v(" "),t("h2",{staticClass:"subtitle has-text-centered"},[t("span",{staticClass:"dnerf"},[e._v("Network Design of the proposed MAM")])])]),e._v(" "),t("p",[e._v("\n                    We investigate how humans make predictions without reference to the original faces by scrutinizing the retouched images. Besides geometric distortion or noise-level artifacts left by retouching algorithms, we find that the other critical factor relies on the features that can be learnt from multiple granularities. For instance, given an image with large eyes, a closer look on the eyes would easily lead to the conclusion that the image has undergone eye-enlarging. However, further considering the large occupation of the face in the image as well as the reasonable ratio of eyes and face, we would reconsider the image as not being eye-enlarged. Similar phenomenon could also be observed on other retouching types. Besides, there can exist a non-negligible amount of spatial redundancy within the visual representations. For example, the background and skin regions can be reduced into two tokenized representation containing the averaged statistic of lightning condition and sharpness. \n                ")]),e._v(" "),t("p",[e._v("\n                    For spatial redundancy reduction, we propose the adaptive token clustering method. For enhanced multi-granularity representation learning, we employ a lightweight two-layered Transformer encoder to analyze and compare multi-granularity information for detection.\n                ")])])])])]),e._v(" "),t("section",{staticClass:"section",attrs:{id:"BibTeX"}},[t("div",{staticClass:"container is-max-desktop content"},[t("h2",{staticClass:"title publication-title"},[e._v("BibTeX")]),e._v(" "),t("pre",[e._v("        "),t("code",[e._v("\n        @article{ying2023retouching,\n        title={RetouchingFFHQ: A Large-scale Dataset for Fine-grained Face Retouching Detection},\n        author={Qichao, Ying and Jiaxin, Liu and Sheng, Li and Haisheng, Xu and Zhenxing, Qian and Xinpeng, Zhang},\n        journal={Proceedings of the 31th ACM International Conference on Multimedia},\n        year={2023}\n        }\n        ")]),e._v("\n    ")])])])])}),[],!1,null,null,null);t.default=n.exports}}]);